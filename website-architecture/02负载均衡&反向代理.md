将域名->多IP，若某服务器重启或发生故障时，切换时间长（因为DNS有一定缓存时间）且没有对后端服务心跳检查和失败重试的机制。可考虑HaProxy和Nginx。
注：外网DNS应用来全局负载均衡，将用户分配到离最近服务器（站长之家的"DNS查询"），内网DNS可实现简单的轮询负载均衡。  

Nginx的吞吐量有一定限制，可在NDS和Nginx间引接入层，eg.LVS(软件负载均衡器)、F5（硬负载均衡器）。  
**流程：** 浏览器访问某域名->DNS解析IP->把IP解析到LVS/F5->转发给Nginx->转发给后端real server。
一般场景下，Nginx（接入层、反向代理|负载均衡服务器）可取代HaProxy。上游服务器（upstream server）指负载均衡到的真实处理业务的服务器（real server）。

## 1. upstream配置
```
http {
    upstream backend{ 
        server 192.168.40.21:8080 weight=1;
        server 192.168.40.21:8081 weight=2;
    }
    location / {
        proxy_pass http://backend
    }
}
```
>每3次请求，有1个转发给8080,2个给8081。当访问Nginx时，会将请求代理到backend配置的upstream server。


## 2. 负载均衡算法
>①round-robin：默认，基于权重的轮询；②ip_hash：根据客户ip将相同ip负载均衡到同一个upstream server;③hash key [consistent]：对某一个key哈希或一致性哈希；④least_conn：负载均衡到最少活跃连接的upstream server，若server数较少，转而使用默认。
>>哈希算法，add/delete服务器，很多key被重新负载均衡到不同服务器，可能出问题；一致性哈希算法，只有少数key被重新负载均衡到不同服务器，推荐。
```
upstream backend{ 
    hash $uri;
    server 192.168.40.21:8080 weight=1;
    ...
}
upstream backend{ 
    hash $consistent_key consistent;
    server 192.168.40.21:8080 weight=1;
    ...
}
location / {
    set $consistent_key $arg_cat;
	if($consistent_key = ''){
	    set $consistent_key $request_uri;
	}
}
```
location会优先考虑请求参数cat（类目），如果没有，再根据请求uri负载均衡。

## 4. 健康检查
>默认采用惰性策略，可集成nginx_upstream_check_module模块主动进行健康检查（支持TCP和HTTP心跳）。

- TCP心跳检查
```
upstream backend{ 
	server 192.168.40.21:8080 weight=1;
	server 192.168.40.21:8081 weight=2;
	check interval=3000 rise=1 fall=3 timeout=2000 type=tcp；
}
```
>每隔3s检测1次，检测成功1次则标识为存活，检测失败3次则标识为不存活，请求超时2s

- HTTP心跳检查
```
upstream backend{ 
	server 192.168.40.21:8080 weight=1;
	server 192.168.40.21:8081 weight=2;
	check interval=3000 rise=1 fall=3 timeout=2000 type=http；
	check_http_send "HEAD /status HTTP /1.0\r\n\r\n";
	check_http_expect_alive http_2xx http_3xx;
}
```
>两个额外配置check_http_send（检查时发的请求内容）check_http_expect_alive（upstream server返回匹配的状态码，则标识存活）。

## 5. 其他配置
- 域名上游服务器
```
upstream backend{ 
	server c0.3.cn;
	server c1.3.cn;
}
```
>当域名ip变更，社区版（解析配置文件阶段将域名解析成IP并记录到stream上,不能更新）&商业版（才能动态更新）。

- 备份上游服务器
```
upstream backend{ 
	server 192.168.40.21:8080 weight=1;
	server 192.168.40.21:8081 weight=2 backup;
}
```
>当所有主upstream server都不存活，会转发给8081端口的备upstream server。压测时，为保险起见配置。

- 不可用上游服务器
```
upstream backend{ 
	server 192.168.40.21:8080 weight=1;
	server 192.168.40.21:8081 weight=2 down;
}
```
>当测试或机器故障，down可暂时摘掉机器。

## 6. 长连接
>Nginx与upstream的长连接,keepalive配置长连接数量（可缓存的最大空闲连接），超过则关闭最近最少使用的连接。
```
upstream backend{ 
	server 192.168.40.21:8080 weight=1;
	server 192.168.40.21:8081 weight=2;
	#开启长连接支持
	keepalive 100;
}
location /test {
	#支持keepalive,与upstream server建立长连接
	proxy_http_version 1.1;
	proxy_set_header Connection "";
    	proxy_pass http://backend;
}
```
>**Nginx实现keepalive思路：** ①询问负载均衡使用哪台服务器(IP+端口)；②轮询空闲连接池（若池中缓存的IP&端口和负载均衡到的一样，则用该连接；然后从池中移除该连接并压入释放连接池栈顶）；③若空闲连接池无可用长连接，则建短连接。  
**释放连接思路：** ①释放连接池无待释放连接，即当创建连接数>连接池大小(出现震荡)；②<连接池大小，从释放池中释放一个连接；③将当前连接压入空闲连接池栈顶。

>连接池一定要根据实际场景：①太小，连接不够用，不断建连接；②太大，空闲连接太多，还没用就超时。（推荐，只对小报文开启）

## 7. HTTP反向代理示例
实现负载均衡+提供缓存  

[Nginx proxy_cache模块的使用记录](http://www.cnblogs.com/hukey/p/5509604.html) 

- 全局配置proxy cache
```
#开启proxy buffer
proxy_buffering on;
proxy_buffer_size 4k;
proxy_buffers 512 4k;
proxy_busy_buffers_size 64k;
proxy_temp_file_write_size 256k;
proxy_cache_lock on;
proxy_cache_lock_timeout 200ms;
#缓存内容存放在tmpfs（内存文件系统）以提升性能
#proxy_temp_path /tmpfs/proxy_temp;
proxy_cache_path /usr/local/nginx/cache/proxy_cache_dir levels=1:2 keys_zone=cache_one:512m inactive=5m max_size=8g;

proxy_connect_timeout 3s;
proxy_send_timeout 5s;
proxy_read_timeout 5s;
```
- location配置
```
location ~ ^/backen/(.*)${
    #设置一致性哈希负载均衡key
    set_by_lus_file $consistent_key "/export/app/c.3.cn/lua/lua_balancing_backend.properties";
    #失败重试
    proxy_next_upstream error timeout;
    proxy_next_upstream_timeout 10s;
    proxy_next_upstream_reties 2;
    #请求上游服务器使用GET方法
    proxy_method GET;
    #不给upstream server发请求体，使upstream server不需解析
    proxy_pass_request_body off；
    #不给upstream server发请求头，使upstream server不受请求头攻击，可proxy_set_header按需传递即可。
    proxy_pass_request_headers off;
    #支持keepalive
    proxy_http_version 1.1;
    proxy_set_header Connection "";
    #给上游服务器传递Referer、Cookie和Host
    proxy_set_header Referer $http_referer;
    proxy_set_header Cookie $http_cookie;
    proxy_set_header Host web.c.3.local;
    proxy_pass http://backend /$1is_args$args;;
}


#内容型响应建议开启
gzip  on;
gzip_min_length 1k;
gzip_buffers 16 16k;
gzip_http_version 1.0;
gzip_proxied any;
#压缩级别根据实际压测决定（带宽&吞吐量抉择）
gzip_comp_level 2;
gzip_vary on;
gzip_types text/plain text/css application/json application/x-javascript application/xml;
```