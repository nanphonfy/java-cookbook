将域名->多IP，若某服务器重启或发生故障时，切换时间长（因为DNS有一定缓存时间）且没有对后端服务心跳检查和失败重试的机制。可考虑HaProxy和Nginx。
注：外网DNS应用来全局负载均衡，将用户分配到离最近服务器（站长之家的"DNS查询"），内网DNS可实现简单的轮询负载均衡。  

Nginx的吞吐量有一定限制，可在NDS和Nginx间引接入层，eg.LVS(软件负载均衡器)、F5（硬负载均衡器）。  
**流程：** 浏览器访问某域名->DNS解析IP->把IP解析到LVS/F5->转发给Nginx->转发给后端real server。
一般场景下，Nginx（接入层、反向代理|负载均衡服务器）可取代HaProxy。上游服务器（upstream server）指负载均衡到的真实处理业务的服务器（real server）。

## 1. upstream配置
```
http {
    upstream backend{ 
        server 192.168.40.21:8080 weight=1;
        server 192.168.40.21:8081 weight=2;
    }
    location / {
        proxy_pass http://backend
    }
}
```
>每3次请求，有1个转发给8080,2个给8081。当访问Nginx时，会将请求代理到backend配置的upstream server。


## 2. 负载均衡算法
>①round-robin：默认，基于权重的轮询；②ip_hash：根据客户ip将相同ip负载均衡到同一个upstream server;③hash key [consistent]：对某一个key哈希或一致性哈希；④least_conn：负载均衡到最少活跃连接的upstream server，若server数较少，转而使用默认。
>>哈希算法，add/delete服务器，很多key被重新负载均衡到不同服务器，可能出问题；一致性哈希算法，只有少数key被重新负载均衡到不同服务器，推荐。
```
upstream backend{ 
    hash $uri;
    server 192.168.40.21:8080 weight=1;
    ...
}
upstream backend{ 
    hash $consistent_key consistent;
    server 192.168.40.21:8080 weight=1;
    ...
}
location / {
    set $consistent_key $arg_cat;
	if($consistent_key = ''){
	    set $consistent_key $request_uri;
	}
}
```
location会优先考虑请求参数cat（类目），如果没有，再根据请求uri负载均衡。

## 4. 健康检查
>默认采用惰性策略，可集成nginx_upstream_check_module模块主动进行健康检查（支持TCP和HTTP心跳）。

- TCP心跳检查
```
upstream backend{ 
	server 192.168.40.21:8080 weight=1;
	server 192.168.40.21:8081 weight=2;
	check interval=3000 rise=1 fall=3 timeout=2000 type=tcp；
}
```
>每隔3s检测1次，检测成功1次则标识为存活，检测失败3次则标识为不存活，请求超时2s

- HTTP心跳检查
```
upstream backend{ 
	server 192.168.40.21:8080 weight=1;
	server 192.168.40.21:8081 weight=2;
	check interval=3000 rise=1 fall=3 timeout=2000 type=http；
	check_http_send "HEAD /status HTTP /1.0\r\n\r\n";
	check_http_expect_alive http_2xx http_3xx;
}
```
>两个额外配置check_http_send（检查时发的请求内容）check_http_expect_alive（upstream server返回匹配的状态码，则标识存活）。

## 5. 其他配置
- 域名上游服务器
```
upstream backend{ 
	server c0.3.cn;
	server c1.3.cn;
}
```
>当域名ip变更，社区版（解析配置文件阶段将域名解析成IP并记录到stream上,不能更新）&商业版（才能动态更新）。

- 备份上游服务器
```
upstream backend{ 
	server 192.168.40.21:8080 weight=1;
	server 192.168.40.21:8081 weight=2 backup;
}
```
>当所有主upstream server都不存活，会转发给8081端口的备upstream server。压测时，为保险起见配置。

- 不可用上游服务器
```
upstream backend{ 
	server 192.168.40.21:8080 weight=1;
	server 192.168.40.21:8081 weight=2 down;
}
```
>当测试或机器故障，down可暂时摘掉机器。

## 6. 长连接
>Nginx与upstream的长连接,keepalive配置长连接数量（可缓存的最大空闲连接），超过则关闭最近最少使用的连接。
```
upstream backend{ 
	server 192.168.40.21:8080 weight=1;
	server 192.168.40.21:8081 weight=2;
	#开启长连接支持
	keepalive 100;
}
location /test {
	#支持keepalive,与upstream server建立长连接
	proxy_http_version 1.1;
	proxy_set_header Connection "";
    	proxy_pass http://backend;
}
```
>**Nginx实现keepalive思路：** ①询问负载均衡使用哪台服务器(IP+端口)；②轮询空闲连接池（若池中缓存的IP&端口和负载均衡到的一样，则用该连接；然后从池中移除该连接并压入释放连接池栈顶）；③若空闲连接池无可用长连接，则建短连接。  
**释放连接思路：** ①释放连接池无待释放连接，即当创建连接数>连接池大小(出现震荡)；②<连接池大小，从释放池中释放一个连接；③将当前连接压入空闲连接池栈顶。

>连接池一定要根据实际场景：①太小，连接不够用，不断建连接；②太大，空闲连接太多，还没用就超时。（推荐，只对小报文开启）

## 7. HTTP反向代理示例
实现负载均衡+提供缓存  

[Nginx proxy_cache模块的使用记录](http://www.cnblogs.com/hukey/p/5509604.html) 

- 全局配置proxy cache
```
#开启proxy buffer
proxy_buffering on;
proxy_buffer_size 4k;
proxy_buffers 512 4k;
proxy_busy_buffers_size 64k;
proxy_temp_file_write_size 256k;
proxy_cache_lock on;
proxy_cache_lock_timeout 200ms;
#缓存内容存放在tmpfs（内存文件系统）以提升性能
#proxy_temp_path /tmpfs/proxy_temp;
proxy_cache_path /usr/local/nginx/cache/proxy_cache_dir levels=1:2 keys_zone=cache_one:512m inactive=5m max_size=8g;

proxy_connect_timeout 3s;
proxy_send_timeout 5s;
proxy_read_timeout 5s;
```
- location配置
```
location ~ ^/backen/(.*)${
    #设置一致性哈希负载均衡key
    set_by_lus_file $consistent_key "/export/app/c.3.cn/lua/lua_balancing_backend.properties";
    #失败重试
    proxy_next_upstream error timeout;
    proxy_next_upstream_timeout 10s;
    proxy_next_upstream_reties 2;
    #请求上游服务器使用GET方法
    proxy_method GET;
    #不给upstream server发请求体，使upstream server不需解析
    proxy_pass_request_body off；
    #不给upstream server发请求头，使upstream server不受请求头攻击，可proxy_set_header按需传递即可。
    proxy_pass_request_headers off;
    #支持keepalive
    proxy_http_version 1.1;
    proxy_set_header Connection "";
    #给上游服务器传递Referer、Cookie和Host
    proxy_set_header Referer $http_referer;
    proxy_set_header Cookie $http_cookie;
    proxy_set_header Host web.c.3.local;
    proxy_pass http://backend /$1is_args$args;
}


#内容型响应建议开启
gzip  on;
gzip_min_length 1k;
gzip_buffers 16 16k;
gzip_http_version 1.0;
gzip_proxied any;
#压缩级别根据实际压测决定（带宽&吞吐量抉择）
gzip_comp_level 2;
gzip_vary on;
gzip_types text/plain text/css application/json application/x-javascript application/xml;
```

## 8. HTTP动态负载均衡
>upstream有变更，线上无法自动注册到Nginx upstream列表，**可使用Consul实现upstream服务的自动发现**，其特性：  
①服务注册到Consul；②从Consul获取服务ip&port；③故障检测，自动摘除；④K/V存储，动态配置（http长轮询触发更改）；⑤多数据中心；⑥raft算法，集群数据一致性。

### 8.1 Consul+consul-template（官方提供）
>启动upstream服务->通过管理后台向Consule注册服务，
Nginx部署启动Consul-template agent（长轮询监听变更），动态修改upstream列表，最后调用重启脚本，重启Nginx(若没启动，则启动)。
- restart.sh
```
ps -fe|grep nginx|grep -v grep
if[$? -ne 0]
then 
 sudo /usr/servers/nginx/sbin/nginx
 echo "nginx start"
else
 sudo /usr/servers/nginx/sbin/nginx -s reload
 echo "nginx reload"
fi
```

### 8.2 Consul+OpenResty
- 实现无reload的动态负载均衡  
>①通过upstream server启动/停止时注册服务或通过管理后台；  
②Nginx启动时调用init_by_lua（拉取配置，把upstream更新到共享字典），然后init_worker_by_lua启动定时器（定期从Consul Server拉取配置并实时更新共享字典）；  
③balance_by_lua对共享字典的upstream动态负载均衡。

## 9. Nginx四层负载均衡
>1.9.0版本后支持，一般场景可用Nginx一站式解决。在实际应用，更多的是用HaProxy进行四层负载（要结合压测情况选择）。
### 9.1 静态负载均衡
>安装Nginx时，添加--with-stream启用（默认，ngx_stream_core_module没启用）
eg. `./configure --prefix=/user/servers --with-stream`
- stream指令
```
stream{
	upstream mysql_backend{
		...
	}
	server{
		...
	}
}
```
- upstream配置
>进行失败重试、惰性健康检查、负载均衡算法等配置
```
upstream backend{ 
	server 192.168.40.21:8080 max_fails=2 fail_timeout=10s weight=1;
	server 192.168.40.21:8081 max_fails=2 fail_timeout=10s weight=2;
	least_conn
}
```
- server配置
```
server{
    #监听端口，默认TCP协议
    listen 3308
    #需用UDP，可配为"listen 3308 udp;"
    #失败重试
    proxy_next_upstream on;
    #与上游服务器连接超时时间，默认60s
    proxy_next_upstream_timeout 10s;
    proxy_next_upstream_reties 2;
    #超时配置
    proxy_connect_timeout 3s;
    #与客户端或上游服务器的两次成功读/写超时时间，可释放不活跃连接，默认10m
    proxy_timeout 1m;
    #限速配置，单位每秒字节数，默认0（不限速）
    proxy_upload_rate 0;
    proxy_download_rate 0；
    #上游服务器
    proxy_pass mysql_backend;
}
```
>当我们重启Nginx服务时，会看到woker进程一直不退出，因为它维持的长连接一直在使用，解决办法：只能杀掉该进程。

### 9.2 动态负载均衡
>nginx-upsync-module提供了HTTP七层动态负载均衡，可动态更新而不需reload Nginx。  

下载并添加：  
eg. `./configure --prefix=/user/servers --with-stream -add-module=./nginx-stream-upsync-module`

- upstream配置
```
upstream backend{ 
    #占位server
    server 192.168.40.21:1111;
    #upsync指定从consul哪个路径拉取upstream server配置
    #从consul拉取配置的超时时间，从consul拉取配置的间隔时间，指定使用consul配置服务器，强制依赖为on时（拉取配置失败，nginx启动也失败）
    upsync 192.168.40.21:8500/v1/kv/upstreams/mysql_backend upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=off;
    #拉取consul，并持久化本地（consul服务器有问题，有备份）
    upsync_dump_path /user/servers/nginx/conf/mysql_backend.conf;
}
```