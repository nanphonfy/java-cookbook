设计系统时的墨菲定律：①任何事都没表面看起来的简单；②所有事都会比你预计的时间长；③可能出错，总会出错；④担心发生，更可能发生。  
系统拆分的康威定律：①架构反映公司；②应按业务闭环进行系统、组织架构划分，闭环、高内聚、低耦合；③若沟通有问题，考虑系统和组织结构调整；④不要一开始把系统、服务拆分得非常细（维护成本高）。

若起初遇到的不是核心问题，别复杂化设计，但先行规划和设计是有必要的。

## 1. 高并发原则

### 1.1 无状态
online：应用无状态（较容易水平扩展），配置文件（不同机房需读取不同数据源）有状态。

### 1.2 拆分
访问量非常大，投入资源较充足，可考虑按功能拆分系统。
>**维度**
>>系统：功能业务拆分，eg.购物车、结算、订单系统等；  
功能：对一个系统再拆分，eg.优惠券系统（后台券创建系统、领券系统、用券系统等）；  
读写：商品系统拆分为写服务（量太大时分库分表）、读服务（配合缓存）。聚合读取（eg.商品详情页），可将分散在多处的数据聚合到一块存储（数据异构数据库系统）；  
AOP：eg.商品详情页可分为CDN（AOP系统）、页面渲染系统；  
模块：eg.基础模块分库分表、数据库连接池，代码结构三层架构划分。

### 1.3 服务化
进程内服务->单机远程服务->集群手动注册服务->自动注册和发现服务（随着调用方越来越多，eg.dubbo使用zookeeper）->服务的分组+隔离（有的系统访问量太大）、路由（动态切换到不同分组）->服务治理如限流、黑白名单（调用量增加）。

### 1.4 消息队列
服务解耦（一对多消费）、异步处理、流量削峰、缓冲，用来解耦一些不需同步调用的服务或订阅一些自己系统关心的。若订阅者太多，单个队列成为瓶颈，可对队列进行多镜像复制；对于生产消息失败，在不能容忍的业务场景，一定要做持久化数据时增加日志、报警等；对于消息重复接收，需在业务层防重处理。
- 大流量缓冲
>促销活动时系统流量会高于正常流量几倍几十倍，此时牺牲强一致性，保证最终一致性即可。
eg.扣减库存时，直接在redis扣减，然后记录扣减日志，通过worker同步到库存DB。  
eg.交易订单系统，结算服务->接单服务->订单redis（用户单个订单详情）&订单队列表（可水平扩展多表，队列缓冲表提升接单能力），然后同步worker把缓冲表同步到订单中心表。若用户支付了订单，则订单状态机变更，此时可能订单队列表还没同步到订单中心表，状态机要重试。
>>同步woker设计时，要考虑并发处理和重复处理问题(单机串行or集群)，还要考虑是否对订单队列添加字段：处理人、处理状态、最后处理时间（应对超时）、失败次数等。
- 数据校对
>消息异步场景下，可能会丢失消息。可通过worker定期扫描原始表，对业务数据校对和修正补偿，来保证数据一致性与完整性。

### 1.5 数据异构
- 数据异构
>订单分库分表一般按订单ID切分，查询某用户订单列表需聚合多个表数据，导致读性能很低。此时需对订单表异构一套用户订单表（按user_id分库分表）。需对历史订单数据进行归档处理。eg.库存价格用数据异构意义不大，可考虑异步加载或合并并发请求。
- 数据闭环
>eg.商品详情页数据来源太多，影响服务稳定性的因素就非常多。解决方案：异构存储，形成数据闭环，闭环和异构其实是一个概念，目的都是实现数据的自我控制。
>>①MQ接收数据变更（一般通过消息队列分发数据），取出多数据源数据；
②数据聚合：可选，把多数据源数据进行聚合，then原子化存储到eg.redis或持久化kv存储；
③前端展示时，任何依赖系统出问题了，还是能正常工作（更新会有积压）。

### 1.6 缓存银弹
- 浏览器端缓存
>设置请求过期时间，eg.Expires、Cache-control，适用于对实时性不太敏感的数据，eg.商品详情页框架、商家评分、评价、广告词等（价格、库存对实时性要求较高）。

- app客户端缓存
>大促为防瞬间流量冲击，会在大促前把app需要访问的素材提前下发到客户端缓存。首屏数据也可缓存，在网络异常时还是有托底数据展示，地图也会做离线缓存。

- CDN缓存
>让用户能在离最近节点取到数据，两种机制：①推送（内容变更后主动推送到CDN边缘节点）和拉取（先访问边缘节点，无内容时，回源到源服务器拿到内容并存储到节点）。注：url有随机数，每次都穿透CDN回源到源服务器相当于CDN无效。爬虫，可返回过期数据而不回源。

- 接入层缓存
>无CDN缓存时，可考虑Nginx，**机制有：**  
①url重写：指定顺序|格式重写，去除随机数；
②一致性哈希：eg.分类|商品编号一致性hash，相同数据落在同一台机器；
③proxy_cache：内存级|SSD级代理缓存；
④proxy_cache_lock：lock机制(设置lock超时时间)，将多数据源合并；
⑤shared_dict：Nginx+lua，可使用lua shared_dict，reload缓存不丢失。

- 应用层缓存
>Tomcat可用堆内缓存（重启丢失，流量风暴扛不住），堆外缓存（也可考虑local redis cache，直接读本机redis，多机间主从同步）。

- 分布式缓存
>local redis cache架构最优，但若数据量太大，则要分片（一致性哈希）分散流量到多台或直接分布式缓存(接入层Nginx+lua读本地缓存，没命中则读分布式redis集群，还没命中，回源到Tomcat读堆内缓存，都没命中，调用业务并获取数据异步写入redis集群)。

### 1.7 并发化

目标数据 | 数据A|数据B|数据C|数据D|数据E
---|---|---|---|---|---
获取时间(ms) | 10|15|20|5|10

串行获取，需60ms。
若C依赖A、B，E依赖C，并发化获取，需30ms。  
A B D  
C  
E  

## 2. 高可用原则
分流：负载均衡+反向代理；
限流：防雪崩；
降级：实现部分可用、有损服务；
隔离：故障隔离；
超时重试：防请求堆积雪崩；
回滚：快速恢复错误版本。

### 2.1 降级
>降级开关设计：  
①开关集中化管理：推送机制把开关推送到各应用；②可降级的多级读服务：eg.1-本地缓存，2-redis缓存，3-降级托底数据；③开关前置化：Nginx层做开关，降级后不回源Tomcat集群|只有一小部分回源；④业务降级：对付高并发流量，可把同步改为异步调用，优先处理高优先级数据，保障数据最终一致性即可。

### 2.2 限流
>防恶意请求、攻击|防超系统流量峰值。原则：限制流量穿透到后端薄弱应用层。  
思路：①恶意请求只访问到cache；②Nginx的limit模块限制穿透到后端的流量；③Nginx deny对恶意ip屏蔽。

### 2.3 切流量
>应对某机房|某机架|某服务器挂了。  
思路：①DNS切换机房入口；②httpDNS在客户端分配好流量入口（调度更精确），绕过运营商localDNS；③LVS/HaProxy切换故障的Nginx接入层（稍麻烦）；④Nginx切换故障应用层（更方便）。

### 2.4 可回滚
>当程序或数据出错，可回滚恢复到最近一个正确版本。eg.事务、代码库、部署版本、数据版本、静态资源版本的回滚等。

## 3 业务设计原则

### 3.1 防重设计
>eg.结算页重复提交、下单重复扣减库存，可用防重key、防重表（注：多支付方式中，渠道不同是无法防止重复提交的，需记录每笔支付情况）。

### 3.2 幂等设计
>现有消息中间件的重复消息消费，需在业务系统做幂等处理。第三方支付的异步回调也要做好回调的幂等处理。

### 3.3 流程可定义
>eg.保险业务中的承保流程和理赔流程分离，需要时关联（可复用一些理赔流程+个性化）。

### 3.4 状态与状态机
>eg.订单交易的状态设计要有状态轨迹（eg.状态变迁：待付款、已付款、待发货、已发货、完成、取消、退款，当状态很多时，使用状态机能更好控制状态迁移），方便用户跟踪当前订单并记录日志，出问题可回溯。  
注意：并发修改状态问题（订单只能有一个修改），状态变更的有序问题，状态变更消息的先后先到问题(eg.支付成功和取消消息的时间差)。

### 3.5 后台操作可反馈
>修改内容要考虑效果的可预览、可反馈（修改规则的数据反馈）。

### 3.6 后台审批化
>某些重要功能需设计审批流，eg.调整价格（日志记录，可追溯、可审计）。

### 3.7 文档和注释
>系统开发一开始就得有文档库（设计架构、设计思想、数据字典|业务流程、现有问题），业务代码和特殊需求都要有注释。

### 3.8 备份
>代码备份和人员备份（一个系统至少应有2名开发人员了解，防离职）。


## 4. 总结图

[高并发图片](http://img.blog.csdn.net/20170909193606583?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZm9yZXZlcmxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

[高可用图片](http://img.blog.csdn.net/20170909193533287?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZm9yZXZlcmxpbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

